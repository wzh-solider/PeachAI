{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#os ： OS模块提供了非常丰富的方法用来处理文件和目录。\n",
    "#sys：sys模块提供了一系列有关Python运行环境的变量和函数。\n",
    "#shutil:用于文件拷贝的模块 \n",
    "#numpy：numpy 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。\n",
    "#random：Python中的random模块用于生成随机数。\n",
    "#paddle.vision.datasets：该模块包含数据加载的相关函数，比如可以用来加载常用的数据集等，如mnist。\n",
    "#paddle.vision.transforms:该模块包含对图像进行转换的函数，比如把HWC格式的图片，转变成CHW模式的输入张量。也包含飞桨框架对于图像预处理的方式，可以快速完成常见的图像预处理，如调整色调、对比度，图像大小等；\n",
    "#paddle.io.Dataset:高模块包含了飞桨框架数据加载方式，可以“一键”完成数据的批加载与异步加载。\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import paddle\n",
    "import random\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddle.vision.datasets import DatasetFolder, ImageFolder\n",
    "from paddle.vision import transforms as T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.568380Z",
     "start_time": "2024-04-10T04:07:49.564411Z"
    }
   },
   "id": "340e37c3d4c0e54d",
   "execution_count": 293
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# '''\n",
    "# 参数配置：\n",
    "# 'train_data_dir'是提供的经增强后的原始训练集；\n",
    "# 'test_image_dir'是提供的原始测试集；\n",
    "# 'train_image_dir'和'eval_image_dir'是由原始训练集经拆分后生成的实际训练集和验证集\n",
    "# 'train_list_dir'和'test_list_dir'是生成的txt文件路径\n",
    "# 'saved_model' 存放训练结果的文件夹\n",
    "# '''\n",
    "train_parameters = {\n",
    "    'train_image_dir': './data/splitted_training_data/train_images',\n",
    "    'eval_image_dir': './data/splitted_training_data/eval_images',\n",
    "    'test_image_dir': './data/enhancement_data/test',\n",
    "    'train_data_dir': './data/enhancement_data/train',\n",
    "    'train_list_dir': './data/enhancement_data/train.txt',\n",
    "    'test_list_dir': './data/enhancement_data/test.txt',\n",
    "    'saved_model': './saved_model/'\n",
    "}\n",
    "\n",
    "#数据集的4个类别标签\n",
    "labels = ['R0', 'B1', 'M2', 'S3']\n",
    "labels.sort()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.580575Z",
     "start_time": "2024-04-10T04:07:49.575906Z"
    }
   },
   "id": "b4c6c18e4a0c066d",
   "execution_count": 294
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#准备生成训练集文件名、标签名的txt文件\n",
    "write_file_name = train_parameters['train_list_dir']\n",
    "\n",
    "#以写方式打开write_file_name文件\n",
    "with open(write_file_name, \"w\") as write_file:\n",
    "    #针对不同的分类标签分别录入\n",
    "    for label in labels:\n",
    "        #建立空列表，用于保存图片名\n",
    "        file_list = []\n",
    "        #用于找到该标签路径下的所有图片.\n",
    "        train_txt_dir = train_parameters['train_data_dir'] + '/' + label + '/'\n",
    "\n",
    "        for file_name in os.listdir(train_txt_dir):\n",
    "            dir_name = label\n",
    "            temp_line = dir_name + '/' + file_name + '\\t' + label + '\\n'  # 例如：\"B1/101.png\tB1\"\n",
    "            write_file.write(temp_line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.616531Z",
     "start_time": "2024-04-10T04:07:49.582093Z"
    }
   },
   "id": "7a22f6855f6393e6",
   "execution_count": 295
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#准备生成测试集文件名、标签名的txt文件\n",
    "write_file_name = train_parameters['test_list_dir']\n",
    "\n",
    "#以写方式打开write_file_name文件\n",
    "with open(write_file_name, \"w\") as write_file:\n",
    "    #针对不同的分类标签分别录入\n",
    "    for label in labels:\n",
    "        #建立空列表，用于保存图片名\n",
    "        file_list = []\n",
    "        #用于找到该标签路径下的所有图片.\n",
    "        train_txt_dir = train_parameters['test_image_dir'] + '/' + label + '/'\n",
    "\n",
    "        for file_name in os.listdir(train_txt_dir):\n",
    "            dir_name = label\n",
    "            temp_line = dir_name + '/' + file_name + '\\t' + label + '\\n'  # 例如：\"B1/101.png\tB1\"\n",
    "            write_file.write(temp_line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.625046Z",
     "start_time": "2024-04-10T04:07:49.617542Z"
    }
   },
   "id": "824e3679d30e2822",
   "execution_count": 296
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#判断splitted_training_data文件夹是否存在，如果不存在就新建一个\n",
    "if not os.path.exists('data/splitted_training_data'):\n",
    "    os.makedirs('data/splitted_training_data')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.629780Z",
     "start_time": "2024-04-10T04:07:49.626052Z"
    }
   },
   "id": "4ae867d3577c06b4",
   "execution_count": 297
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#定义一个函数，来拆分训练集、验证集\n",
    "def create_train_eval():\n",
    "    '''\n",
    "    划分训练集和验证集\n",
    "    '''\n",
    "    train_dir = train_parameters['train_image_dir']\n",
    "    eval_dir = train_parameters['eval_image_dir']\n",
    "    train_list_path = train_parameters['train_list_dir']\n",
    "    train_data_dir = train_parameters['train_data_dir']\n",
    "\n",
    "    print('creating training and eval images')\n",
    "    #如果文件夹不存在，建立相应的文件夹\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.mkdir(train_dir)\n",
    "    if not os.path.exists(eval_dir):\n",
    "        os.mkdir(eval_dir)\n",
    "\n",
    "        #打开txt文件，分割数据\n",
    "    file_name = train_list_path\n",
    "    f = open(file_name, 'r')\n",
    "    #按行读取数据\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        #将每行数据按照空格分割成2部分，并取第一部分的路径名和图像文件名，例如:R0/1.png\n",
    "        img_path = lines[i].split('\\t')[0]\n",
    "        #取第二部分的标签，例如:R0\n",
    "        class_label = lines[i].split('\\t')[1].strip('\\n')\n",
    "        # 每8张图片取一个做验证数据,其他用于训练\n",
    "        if i % 8 == 0:\n",
    "            #把目录和文件名合成一个路径\n",
    "            eval_target_dir = os.path.join(eval_dir, class_label)\n",
    "            #将总的文件路径与当前图像的文件名合到一起，实际就是得到训练集图像所在的文件夹下的图像名   \n",
    "            eval_img_path = os.path.join(train_data_dir, img_path)\n",
    "            if not os.path.exists(eval_target_dir):\n",
    "                os.mkdir(eval_target_dir)\n",
    "                #将图片复制到验证集指定标签的文件夹下      \n",
    "            shutil.copy(eval_img_path, eval_target_dir)\n",
    "        else:\n",
    "            train_target_dir = os.path.join(train_dir, class_label)\n",
    "            train_img_path = os.path.join(train_data_dir, img_path)\n",
    "            if not os.path.exists(train_target_dir):\n",
    "                os.mkdir(train_target_dir)\n",
    "            shutil.copy(train_img_path, train_target_dir)\n",
    "    print('划分训练集和验证集完成！')\n",
    "\n",
    "# 制作数据集，如果已经做好了，就请将代码注释掉\n",
    "# create_train_eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.639905Z",
     "start_time": "2024-04-10T04:07:49.632292Z"
    }
   },
   "id": "7322e039ad424623",
   "execution_count": 298
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PeachDataset(Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承paddle.io.Dataset类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        \"\"\"\n",
    "        步骤二：实现构造函数，定义数据读取方式，划分训练、验证和测试数据集\n",
    "        \"\"\"\n",
    "        super(PeachDataset, self).__init__()\n",
    "        train_image_dir = train_parameters['train_image_dir']  #训练集的路径\n",
    "        eval_image_dir = train_parameters['eval_image_dir']\n",
    "        test_image_dir = train_parameters['test_image_dir']\n",
    "\n",
    "        '''         '''\n",
    "        #transform数据增强函数，这里仅对图片的打开方式进行了转换            \n",
    "        #这里用Transpose()将图片的打开方式(宽, 高, 通道数)更改为PaddlePaddle读取的方式是(通道数, 宽, 高)\n",
    "        mean = [127.5, 127.5, 127.5]  # 归一化，均值\n",
    "        std = [127.5, 127.5, 127.5]  # 归一化，标注差 \n",
    "        transform_train = T.Compose([T.ColorJitter(0.4, 0.4, 0.4, 0.4)\n",
    "                                        , T.Resize(size=(224, 224))\n",
    "                                        , T.Transpose()\n",
    "                                        , T.Normalize(mean, std)\n",
    "                                     ])\n",
    "        transform_eval = T.Compose([T.Resize(size=(224, 224))\n",
    "                                       , T.Transpose()\n",
    "                                       , T.Normalize(mean, std)\n",
    "                                    ])\n",
    "        transform_test = T.Compose([T.Resize(size=(224, 224))\n",
    "                                       , T.Transpose()\n",
    "                                       , T.Normalize(mean, std)\n",
    "                                    ])\n",
    "\n",
    "        '''         \n",
    "        # 参考API：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/Overview_cn.html#about-transforms\n",
    "        #这里用Transpose()将图片的打开方式(宽, 高, 通道数)更改为PaddlePaddle读取的方式是(通道数, 宽, 高)\n",
    "        # ColorJitter 随机调整图像的亮度，对比度，饱和度和色调。\n",
    "        # hflip 对输入图像进行水平翻转。        \n",
    "        # Normalize 归一化。mean = [127.5, 127.5, 127.5]，std = [127.5, 127.5, 127.5]\n",
    "        # RandomHorizontalFlip 基于概率来执行图片的水平翻转。\n",
    "        # RandomVerticalFlip 基于概率来执行图片的垂直翻转。\n",
    "        mean = [127.5, 127.5, 127.5] # 归一化，均值\n",
    "        std = [127.5, 127.5, 127.5] # 归一化，标注差 \n",
    "        transform_train = T.Compose([T.Resize(size=(224,224)), \n",
    "                                     T.Transpose(),                                \n",
    "                                     T.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
    "                                     T.RandomHorizontalFlip(prob=0.5,),\n",
    "                                     T.RandomVerticalFlip(prob=0.5,),\n",
    "                                     T.Normalize(mean, std)])\n",
    "        transform_eval = T.Compose([T.Resize(size=(224,224)), T.Transpose()])\n",
    "        transform_test = T.Compose([T.Resize(size=(224,224)), T.Transpose()])\n",
    "        '''\n",
    "\n",
    "        #飞桨推荐使用 paddle.io.DataLoader 完成数据的加载，生成一个可以加载数据的迭代器\n",
    "        # 参考API:https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html#cn-api-fluid-io-dataloader\n",
    "        #加载训练集，train_data_folder 是一个迭代器\n",
    "        # 参考API：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/datasets/DatasetFolder_cn.html#datasetfolder\n",
    "        train_data_folder = DatasetFolder(train_image_dir, transform=transform_train)\n",
    "        #加载验证集，eval_data_folder 是一个迭代器\n",
    "        eval_data_folder = DatasetFolder(eval_image_dir, transform=transform_eval)\n",
    "        #加载测试集，test_data_folder 是一个迭代器\n",
    "        test_data_folder = DatasetFolder(test_image_dir, transform=transform_test)\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.data = train_data_folder\n",
    "        elif self.mode == 'eval':\n",
    "            self.data = eval_data_folder\n",
    "        elif self.mode == 'test':\n",
    "            self.data = test_data_folder\n",
    "\n",
    "    # 每次迭代时返回数据和对应的标签\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\n",
    "        \"\"\"\n",
    "        data = np.array(self.data[index][0]).astype('float32')\n",
    "\n",
    "        label = np.array([self.data[index][1]]).astype('int64')\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    # 返回整个数据集的总数\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现__len__方法，返回数据集总数目\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.651455Z",
     "start_time": "2024-04-10T04:07:49.640911Z"
    }
   },
   "id": "c458ab62832e2857",
   "execution_count": 299
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#用自定义的PeachDataset类，加载自己的数据集\n",
    "train_dataset = PeachDataset(mode='train')\n",
    "val_dataset = PeachDataset(mode='eval')\n",
    "test_dataset = PeachDataset(mode='test')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.777220Z",
     "start_time": "2024-04-10T04:07:49.652969Z"
    }
   },
   "id": "8276f74e1dd0f1b8",
   "execution_count": 300
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opencv 版本号为：4.9.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DataLoader 示例代码\n",
    "\n",
    "# 加载库\n",
    "import cv2 as cv  #使用 OpenCV\n",
    "\n",
    "print(\"opencv 版本号为：\" + cv.__version__)  #查看版本号\n",
    "# 事实上在使用 OpenCV之前应该安装该类库，但是由于使用了 AI-Studio，所以系统已经替开发者预先安装好了： opencv-python 4.1.1.26       \n",
    "from matplotlib import pyplot as plt  #在该页面画图\n",
    "%matplotlib inline \n",
    "\n",
    "# 构造一个 DataLoader\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=2,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True,\n",
    "                         num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.785516Z",
     "start_time": "2024-04-10T04:07:49.779226Z"
    }
   },
   "id": "37945b6eb1cae757",
   "execution_count": 301
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch 的类型为：<class 'list'>\n",
      "mini_batch 的大小为：2\n",
      "(3, 224, 224)\n",
      "(3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# 使用 DataLoader 来遍历数据集\n",
    "for mini_batch in test_loader:  # 从 DataLoader 中获取 mini_batch \n",
    "    print(\"mini_batch 的类型为：\" + str(type(mini_batch)))\n",
    "    pic_list = mini_batch[0]  #图片数据\n",
    "    label_list = mini_batch[1]  #标记\n",
    "    print(\"mini_batch 的大小为：\" + str(len(pic_list)))\n",
    "\n",
    "    # 将图片显示转化为 numpy 格式，并且将内部的数字设置为 整数类型\n",
    "    pic_1 = pic_list[0]\n",
    "    pic_2 = pic_list[1]\n",
    "    arr1 = np.asarray(pic_1, dtype=np.float64)\n",
    "    print(arr1.shape)\n",
    "    arr2 = np.asarray(pic_2, dtype=np.float64)\n",
    "    print(arr2.shape)\n",
    "\n",
    "    break  #由于是示例，所以仅拿出第一个 mini_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.819614Z",
     "start_time": "2024-04-10T04:07:49.786521Z"
    }
   },
   "id": "75353e4e8724dc25",
   "execution_count": 302
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 把获取到的图片数据展示出来\n",
    "# arr1 = arr1 / 255 # 把每一个像素都变到 0-1 之间\n",
    "# r = arr1[0]\n",
    "# g = arr1[1]\n",
    "# b = arr1[2]\n",
    "# img = cv.merge([r, g, b])\n",
    "# \n",
    "# plt.imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:49.826299Z",
     "start_time": "2024-04-10T04:07:49.822124Z"
    }
   },
   "id": "95ae01f36911f7db",
   "execution_count": 303
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 使用内置的模型,这边可以选择多种不同网络，这里选了resnet50网络\n",
    "#pretrained (bool，可选) - 是否加载在imagenet数据集上的预训练权重\n",
    "model = paddle.vision.models.resnet18(pretrained=True, num_classes=4)\n",
    "\n",
    "#尝试不同的网络结构：MobileNetV2\n",
    "# MobileNetV2参考文档：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/models/MobileNetV2_cn.html\n",
    "# model = paddle.vision.models.mobilenet_v2(pretrained=True, num_classes=4)    \n",
    "\n",
    "#使用paddle.Model完成模型的封装，将网络结构组合成一个可快速使用高层API进行训练和预测的类。\n",
    "model = paddle.Model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:50.960446Z",
     "start_time": "2024-04-10T04:07:49.828305Z"
    }
   },
   "id": "1e273f2031247d08",
   "execution_count": 304
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    Layer (type)         Input Shape          Output Shape         Param #    \n",
      "================================================================================\n",
      "     Conv2D-241       [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408     \n",
      "  BatchNorm2D-241    [[1, 64, 112, 112]]   [1, 64, 112, 112]         256      \n",
      "      ReLU-109       [[1, 64, 112, 112]]   [1, 64, 112, 112]          0       \n",
      "    MaxPool2D-13     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0       \n",
      "     Conv2D-242       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-242     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-110        [[1, 64, 56, 56]]     [1, 64, 56, 56]           0       \n",
      "     Conv2D-243       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-243     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "   BasicBlock-97      [[1, 64, 56, 56]]     [1, 64, 56, 56]           0       \n",
      "     Conv2D-244       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-244     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-111        [[1, 64, 56, 56]]     [1, 64, 56, 56]           0       \n",
      "     Conv2D-245       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-245     [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "   BasicBlock-98      [[1, 64, 56, 56]]     [1, 64, 56, 56]           0       \n",
      "     Conv2D-247       [[1, 64, 56, 56]]     [1, 128, 28, 28]       73,728     \n",
      "  BatchNorm2D-247     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-112        [[1, 128, 28, 28]]    [1, 128, 28, 28]          0       \n",
      "     Conv2D-248       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-248     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "     Conv2D-246       [[1, 64, 56, 56]]     [1, 128, 28, 28]        8,192     \n",
      "  BatchNorm2D-246     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "   BasicBlock-99      [[1, 64, 56, 56]]     [1, 128, 28, 28]          0       \n",
      "     Conv2D-249       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-249     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-113        [[1, 128, 28, 28]]    [1, 128, 28, 28]          0       \n",
      "     Conv2D-250       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-250     [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "   BasicBlock-100     [[1, 128, 28, 28]]    [1, 128, 28, 28]          0       \n",
      "     Conv2D-252       [[1, 128, 28, 28]]    [1, 256, 14, 14]       294,912    \n",
      "  BatchNorm2D-252     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-114        [[1, 256, 14, 14]]    [1, 256, 14, 14]          0       \n",
      "     Conv2D-253       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-253     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "     Conv2D-251       [[1, 128, 28, 28]]    [1, 256, 14, 14]       32,768     \n",
      "  BatchNorm2D-251     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "   BasicBlock-101     [[1, 128, 28, 28]]    [1, 256, 14, 14]          0       \n",
      "     Conv2D-254       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-254     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-115        [[1, 256, 14, 14]]    [1, 256, 14, 14]          0       \n",
      "     Conv2D-255       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-255     [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "   BasicBlock-102     [[1, 256, 14, 14]]    [1, 256, 14, 14]          0       \n",
      "     Conv2D-257       [[1, 256, 14, 14]]     [1, 512, 7, 7]       1,179,648   \n",
      "  BatchNorm2D-257      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "      ReLU-116         [[1, 512, 7, 7]]      [1, 512, 7, 7]           0       \n",
      "     Conv2D-258        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-258      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "     Conv2D-256       [[1, 256, 14, 14]]     [1, 512, 7, 7]        131,072    \n",
      "  BatchNorm2D-256      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "   BasicBlock-103     [[1, 256, 14, 14]]     [1, 512, 7, 7]           0       \n",
      "     Conv2D-259        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-259      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "      ReLU-117         [[1, 512, 7, 7]]      [1, 512, 7, 7]           0       \n",
      "     Conv2D-260        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-260      [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "   BasicBlock-104      [[1, 512, 7, 7]]      [1, 512, 7, 7]           0       \n",
      "AdaptiveAvgPool2D-13   [[1, 512, 7, 7]]      [1, 512, 1, 1]           0       \n",
      "     Linear-13            [[1, 512]]             [1, 4]             2,052     \n",
      "================================================================================\n",
      "Total params: 11,188,164\n",
      "Trainable params: 11,178,564\n",
      "Non-trainable params: 9,600\n",
      "--------------------------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 57.04\n",
      "Params size (MB): 42.68\n",
      "Estimated Total Size (MB): 100.30\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'total_params': 11188164, 'trainable_params': 11178564}"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 summary 观察网络信息\n",
    "model.summary(input_size=(1, 3, 224, 224), dtype='float32')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:51.069989Z",
     "start_time": "2024-04-10T04:07:50.962040Z"
    }
   },
   "id": "447241548baab581",
   "execution_count": 305
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba14f87476e42c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 调用Paddle的VisualDL模块，保存信息到目录中。\n",
    "#log_dir (str) - 输出日志保存的路径。\n",
    "callback = paddle.callbacks.VisualDL(log_dir='visualdl_log_dir')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:51.074868Z",
     "start_time": "2024-04-10T04:07:51.070993Z"
    }
   },
   "id": "909a9fae18ee48f8",
   "execution_count": 306
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#通过Model.prepare接口来对训练进行提前的配置准备工作，包括设置模型优化器，Loss计算方法，精度计算方法等\n",
    "# 优化器API文档： https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Overview_cn.html#paddle-optimizer\n",
    "\n",
    "# 学习率衰减策略\n",
    "# 学习率衰减策略 API 文档：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Overview_cn.html#about-lr\n",
    "scheduler_StepDecay = paddle.optimizer.lr.StepDecay(learning_rate=0.1, step_size=50, gamma=0.9, verbose=False)\n",
    "scheduler_PiecewiseDecay = paddle.optimizer.lr.PiecewiseDecay(boundaries=[100, 1000, 4000, 5000, 6000],values=[0.1, 0.5, 0.01, 0.005, 0.001, 0.0005], verbose=False)\n",
    "\n",
    "# 尝试使用 SGD、Momentum 方法\n",
    "sgd = paddle.optimizer.SGD(\n",
    "    learning_rate=scheduler_StepDecay,\n",
    "    parameters=model.parameters())\n",
    "\n",
    "adam = paddle.optimizer.Adam(\n",
    "    learning_rate=0.01,  #调参\n",
    "    parameters=model.parameters())\n",
    "\n",
    "model.prepare(optimizer=adam,  # adam\n",
    "              loss=paddle.nn.CrossEntropyLoss(),\n",
    "              metrics=paddle.metric.Accuracy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:51.082910Z",
     "start_time": "2024-04-10T04:07:51.075874Z"
    }
   },
   "id": "747a69731cf6ebc8",
   "execution_count": 307
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Place(cpu)\n"
     ]
    }
   ],
   "source": [
    "# 查看当前计算设备\n",
    "device = paddle.device.get_device()\n",
    "print(device)\n",
    "# 使用CPU训练\n",
    "device = paddle.set_device('cpu')  # or 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T04:07:51.088806Z",
     "start_time": "2024-04-10T04:07:51.083917Z"
    }
   },
   "id": "51f695f8d10b1088",
   "execution_count": 308
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n",
      "step 2010/2904 [===================>..........] - loss: 1.2407 - acc: 0.3438 - ETA: 11:50 - 795ms/step"
     ]
    }
   ],
   "source": [
    "# fit API文档： https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/Model_cn.html#fit-train-data-none-eval-data-none-batch-size-1-epochs-1-eval-freq-1-log-freq-10-save-dir-none-save-freq-1-verbose-2-drop-last-false-shuffle-true-num-workers-0-callbacks-none\n",
    "\n",
    "# 启动模型训练，指定训练数据集，设置训练轮次，设置每次数据集计算的批次大小，设置日志格式\n",
    "#epochs：总共训练的轮数\n",
    "#batch_size：一个批次的样本数量\n",
    "#如果提示内存不足，可以尝试将batch_size调低\n",
    "#verbose：日志显示，0为不在标准输出流输出日志信息,1为输出进度条记录，2为每个epoch输出一行记录;1为输出进度条记录，2为每个epoch输出一行记录\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          val_dataset,\n",
    "          epochs=1,\n",
    "          batch_size=2,\n",
    "          callbacks=callback,\n",
    "          verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-10T04:07:51.090814Z"
    }
   },
   "id": "2e1ddae510334aca",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
